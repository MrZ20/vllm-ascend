#
# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
# This file is a part of the vllm-ascend project.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Generate mapping from source functions to test cases using .coverage data.

Usage:
    python tools/tia/generate_mapping.py \
        --coverage-db .coverage \
        --source-dir vllm_ascend \
        --output mapping_singlecard.json

Input:
    - .coverage database generated by `pytest --cov=vllm_ascend --cov-context=test_function`
    - Source directory vllm_ascend/

Output:
    - mapping_*.json with file_mapping and func_mapping
"""

import argparse
import json
import os
import re
import sqlite3
import sys
from collections import defaultdict
from typing import Any

# Allow running from project root
sys.path.insert(0, os.path.dirname(__file__))
from ast_parser import find_function_for_line, is_utility_file, parse_source_functions

try:
    # coverage.py 官方解码实现（最稳）
    from coverage.numbits import numbits_to_nums as _coverage_numbits_to_nums  # type: ignore
except Exception:  # pragma: no cover
    _coverage_numbits_to_nums = None


def read_coverage_db(coverage_db_path: str) -> list[tuple[str, int, str]]:
    """
    Read coverage data from .coverage SQLite database.

    Returns list of (file_path, line_number, test_context) tuples.
    The .coverage database schema (coverage.py 5.x+):
        - file: table mapping file_id -> path
        - context: table mapping context_id -> context string
        - line_bits: table with file_id, context_id, numbits (bitmap of covered lines)

    For older coverage.py or different formats, we also try the arc-based schema.
    """
    if not os.path.exists(coverage_db_path):
        print(f"ERROR: Coverage database not found: {coverage_db_path}")
        sys.exit(1)

    conn = sqlite3.connect(coverage_db_path)
    cursor = conn.cursor()

    results: list[tuple[str, int, str]] = []

    try:
        # Get file mapping
        cursor.execute("SELECT id, path FROM file")
        files = {row[0]: row[1] for row in cursor.fetchall()}

        # Get context mapping
        cursor.execute("SELECT id, context FROM context")
        contexts = {row[0]: row[1] for row in cursor.fetchall()}

        # Try line_bits table (coverage.py 5.x+ with contexts)
        try:
            cursor.execute("SELECT file_id, context_id, numbits FROM line_bits")
            for file_id, context_id, numbits in cursor.fetchall():
                file_path = files.get(file_id, "")
                context = contexts.get(context_id, "")
                if not file_path or not context:
                    continue

                # Decode numbits bitmap to line numbers
                lines = _numbits_to_lines(bytes(numbits))
                for line_no in lines:
                    results.append((file_path, line_no, context))
        except sqlite3.OperationalError:
            # Fallback: try arc table
            try:
                cursor.execute("SELECT file_id, context_id, fromno, tono FROM arc")
                for file_id, context_id, fromno, tono in cursor.fetchall():
                    file_path = files.get(file_id, "")
                    context = contexts.get(context_id, "")
                    if not file_path or not context:
                        continue
                    # For arc data, both fromno and tono are relevant lines
                    if fromno > 0:
                        results.append((file_path, fromno, context))
                    if tono > 0:
                        results.append((file_path, tono, context))
            except sqlite3.OperationalError:
                print("ERROR: Cannot read coverage data - unsupported database schema")
                sys.exit(1)

    finally:
        conn.close()

    return results


def _numbits_to_lines(numbits: bytes) -> list[int]:
    """
    Convert a coverage.py numbits BLOB to a list of executed line numbers.

    Prefer coverage.numbits.numbits_to_nums when available. Fallback keeps
    consistent 1-based line numbers.
    """
    if _coverage_numbits_to_nums is not None:
        lines = [int(x) for x in _coverage_numbits_to_nums(numbits)]
        return [ln for ln in lines if ln > 0]

    # Fallback: manual bit-walk.
    # IMPORTANT: coverage line numbers are 1-based, so add +1.
    lines: list[int] = []
    for byte_idx, byte_val in enumerate(numbits):
        for bit_idx in range(8):
            if byte_val & (1 << bit_idx):
                ln = byte_idx * 8 + bit_idx + 1
                if ln > 0:
                    lines.append(ln)
    return lines


def normalize_test_context(context: str) -> str:
    """
    Normalize test context string from coverage.py to a clean test identifier.

    Coverage.py contexts look like:
        "tests/e2e/singlecard/test_models.py::test_models[model0]|run"
        "tests/ut/test_utils.py::TestUtils::test_nd_to_nz_2d|setup"

    We strip the |phase suffix and keep the rest.
    """
    # pytest-cov contexts may carry phase suffix like "|run"/"|setup"/"|teardown"/"|call".
    # Keep the nodeid portion and strip a single trailing "|..." segment.
    context = re.sub(r"\|[^|]+$", "", context)
    return context.strip()


def normalize_file_path(file_path: str, project_root: str) -> str:
    """
    Normalize file path to be relative to project root.
    """
    file_path = os.path.abspath(file_path)
    project_root = os.path.abspath(project_root)

    if file_path.startswith(project_root):
        return os.path.relpath(file_path, project_root)
    return file_path


def generate_mapping(
    coverage_db_path: str,
    source_dir: str,
    project_root: str,
) -> dict[str, Any]:
    """
    Generate the complete mapping from source to tests.

    Args:
        coverage_db_path: Path to .coverage SQLite database.
        source_dir: Source directory to map (e.g., "vllm_ascend").
        project_root: Project root directory for path normalization.

    Returns:
        Dictionary with "file_mapping" and "func_mapping" keys.
    """
    coverage_data = read_coverage_db(coverage_db_path)
    print(f"Read {len(coverage_data)} coverage records")

    # Quick sanity check: per-test mapping requires dynamic contexts.
    # If we only see a single non-nodeid context like "test_function", mapping quality will be very low.
    contexts_seen = {ctx for _, _, ctx in coverage_data if ctx}
    if len(contexts_seen) == 1:
        only_ctx = next(iter(contexts_seen))
        if ".py::" not in only_ctx and only_ctx in {"test_function", "pytest"}:
            print(
                "WARNING: .coverage contains a single non-nodeid context. "
                "Did you generate coverage with pytest-cov (pytest --cov-context=test_function)? "
                "Using 'coverage run --context=test_function -m pytest' sets a fixed context and won't map per test."
            )

    # Cache parsed AST results per file
    ast_cache: dict[str, dict[tuple[int, int], str]] = {}

    file_mapping: dict[str, set[str]] = defaultdict(set)
    func_mapping: dict[str, set[str]] = defaultdict(set)

    source_dir_abs = os.path.abspath(os.path.join(project_root, source_dir))

    skipped_no_source = 0
    skipped_no_function = 0
    mapped_file_level = 0
    mapped_func_level = 0

    for file_path, line_number, context in coverage_data:
        # Normalize paths
        rel_path = normalize_file_path(file_path, project_root)
        abs_path = os.path.join(project_root, rel_path)

        # Only process files under source_dir
        if not os.path.abspath(abs_path).startswith(source_dir_abs):
            skipped_no_source += 1
            continue

        # Skip test files themselves
        if "/tests/" in rel_path.replace("\\", "/") or rel_path.startswith("tests/"):
            continue

        test_name = normalize_test_context(context)
        if not test_name:
            continue

        # Decide file-level vs function-level mapping
        if is_utility_file(rel_path):
            file_mapping[rel_path].add(test_name)
            mapped_file_level += 1
        else:
            # Parse AST if not cached
            if rel_path not in ast_cache:
                ast_cache[rel_path] = parse_source_functions(abs_path)

            functions = ast_cache[rel_path]
            func_name = find_function_for_line(functions, line_number)

            if func_name:
                func_key = f"{rel_path}::{func_name}"
                func_mapping[func_key].add(test_name)
                mapped_func_level += 1
            else:
                # Line is at module level (imports, constants, etc.)
                # Map at file level
                file_mapping[rel_path].add(test_name)
                skipped_no_function += 1

    print("Mapping stats:")
    print(f"  File-level mappings: {len(file_mapping)} files, {mapped_file_level} records")
    print(f"  Function-level mappings: {len(func_mapping)} functions, {mapped_func_level} records")
    print(f"  Skipped (not in source): {skipped_no_source}")
    print(f"  Module-level lines (file-mapped): {skipped_no_function}")

    # Convert sets to sorted lists for JSON serialization
    result = {
        "file_mapping": {k: sorted(v) for k, v in sorted(file_mapping.items())},
        "func_mapping": {k: sorted(v) for k, v in sorted(func_mapping.items())},
    }

    return result


def merge_mappings(mapping_files: list[str]) -> dict[str, Any]:
    """
    Merge multiple mapping JSON files into one.

    Args:
        mapping_files: List of paths to mapping_*.json files.

    Returns:
        Merged mapping dictionary.
    """
    merged_file: dict[str, set[str]] = defaultdict(set)
    merged_func: dict[str, set[str]] = defaultdict(set)

    for path in mapping_files:
        with open(path) as f:
            data = json.load(f)

        for key, tests in data.get("file_mapping", {}).items():
            merged_file[key].update(tests)

        for key, tests in data.get("func_mapping", {}).items():
            merged_func[key].update(tests)

    return {
        "file_mapping": {k: sorted(v) for k, v in sorted(merged_file.items())},
        "func_mapping": {k: sorted(v) for k, v in sorted(merged_func.items())},
    }


def main():
    parser = argparse.ArgumentParser(
        description="Generate source-to-test mapping from coverage data"
    )
    subparsers = parser.add_subparsers(dest="command", help="Sub-command")

    # Generate sub-command
    gen_parser = subparsers.add_parser("generate", help="Generate mapping from .coverage")
    gen_parser.add_argument(
        "--coverage-db", required=True, help="Path to .coverage database"
    )
    gen_parser.add_argument(
        "--source-dir",
        default="vllm_ascend",
        help="Source directory to map (default: vllm_ascend)",
    )
    gen_parser.add_argument(
        "--project-root", default=".", help="Project root directory"
    )
    gen_parser.add_argument(
        "--output", required=True, help="Output JSON file path"
    )

    # Merge sub-command
    merge_parser = subparsers.add_parser("merge", help="Merge multiple mapping files")
    merge_parser.add_argument(
        "inputs", nargs="+", help="Input mapping JSON files"
    )
    merge_parser.add_argument(
        "--output", default="mapping.json", help="Output merged JSON file"
    )

    args = parser.parse_args()

    if args.command == "generate":
        mapping = generate_mapping(
            args.coverage_db, args.source_dir, args.project_root
        )
        with open(args.output, "w") as f:
            json.dump(mapping, f, indent=2)
        print(f"Mapping written to {args.output}")
        print(f"  File mappings: {len(mapping['file_mapping'])}")
        print(f"  Function mappings: {len(mapping['func_mapping'])}")

    elif args.command == "merge":
        merged = merge_mappings(args.inputs)
        with open(args.output, "w") as f:
            json.dump(merged, f, indent=2)
        print(f"Merged {len(args.inputs)} files into {args.output}")
        print(f"  File mappings: {len(merged['file_mapping'])}")
        print(f"  Function mappings: {len(merged['func_mapping'])}")

    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
