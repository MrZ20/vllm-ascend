# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file is a part of the vllm-ascend project.
#

# =============================================================================
# Qwen3-32B-W8A8 (INT8 Quantized) Single Node Test Configuration
# =============================================================================
# This configuration runs Qwen3-32B with W8A8 quantization on a single node.
# Test mode: aclgraph (with cudagraph compilation)

test_name: "Qwen3-32B-W8A8 single node test (aclgraph)"
model: "vllm-ascend/Qwen3-32B-W8A8"

# Single node configuration
num_nodes: 1
npu_per_node: 4

# Environment variables
env_common:
  TASK_QUEUE_ENABLE: "1"
  HCCL_OP_EXPANSION_MODE: "AIV"
  VLLM_ASCEND_ENABLE_FLASHCOMM: "1"
  VLLM_ASCEND_ENABLE_PREFETCH_MLP: "1"
  SERVER_PORT: 8080

# Server arguments for aclgraph mode
server_args:
  - "--quantization"
  - "ascend"
  - "--no-enable-prefix-caching"
  - "--tensor-parallel-size"
  - "4"
  - "--max-model-len"
  - "40960"
  - "--max-num-batched-tokens"
  - "40960"
  - "--block-size"
  - "128"
  - "--trust-remote-code"
  - "--reasoning-parser"
  - "qwen3"
  - "--gpu-memory-utilization"
  - "0.9"
  - "--async-scheduling"
  - "--compilation-config"
  - '{"cudagraph_mode": "FULL_DECODE_ONLY", "cudagraph_capture_sizes": [1, 12, 16, 20, 24, 32, 48, 60, 64, 68, 72, 76, 80]}'

# Benchmark configurations
benchmarks:
  - case_type: accuracy
    dataset_path: vllm-ascend/aime2024
    request_conf: vllm_api_general_chat
    dataset_conf: aime2024/aime2024_gen_0_shot_chat_prompt
    max_out_len: 32768
    batch_size: 32
    baseline: 83.33
    threshold: 7

  - case_type: performance
    dataset_path: vllm-ascend/GSM8K-in3500-bs400
    request_conf: vllm_api_stream_chat
    dataset_conf: gsm8k/gsm8k_gen_0_shot_cot_str_perf
    num_prompts: 288  # 4 * 72 for A2, adjust based on platform
    max_out_len: 1500
    batch_size: 72
    baseline: 1
    threshold: 0.97
