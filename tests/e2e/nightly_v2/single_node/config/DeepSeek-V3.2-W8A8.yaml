# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file is a part of the vllm-ascend project.
#

# =============================================================================
# DeepSeek-V3.2-W8A8 Single Node Test Configuration
# =============================================================================
# This configuration runs DeepSeek-V3.2-W8A8 on a single node with 16 NPUs.

test_name: "DeepSeek-V3.2-W8A8 single node test"
model: "vllm-ascend/DeepSeek-V3.2-W8A8"

# Single node configuration
num_nodes: 1
npu_per_node: 16

# Environment variables
env_common:
  HCCL_OP_EXPANSION_MODE: "AIV"
  OMP_PROC_BIND: "false"
  OMP_NUM_THREADS: "1"
  HCCL_BUFFSIZE: "1024"
  VLLM_ASCEND_ENABLE_MLAPO: "1"
  PYTORCH_NPU_ALLOC_CONF: "expandable_segments:True"
  VLLM_ASCEND_ENABLE_FLASHCOMM1: "1"
  SERVER_PORT: 8080

# Server arguments
server_args:
  - "--enable-expert-parallel"
  - "--tensor-parallel-size"
  - "8"
  - "--data-parallel-size"
  - "2"
  - "--max-model-len"
  - "8192"
  - "--max-num-batched-tokens"
  - "8192"
  - "--max-num-seqs"
  - "4"
  - "--trust-remote-code"
  - "--quantization"
  - "ascend"
  - "--gpu-memory-utilization"
  - "0.92"
  - "--compilation-config"
  - '{"cudagraph_capture_sizes":[3, 6, 9, 12], "cudagraph_mode":"FULL_DECODE_ONLY"}'
  - "--speculative-config"
  - '{"num_speculative_tokens": 2, "method":"deepseek_mtp"}'
  - "--additional-config"
  - '{"layer_sharding": ["q_b_proj", "o_proj"]}'
  - "--reasoning-parser"
  - "deepseek_v3"
  - "--tokenizer_mode"
  - "deepseek_v32"

# Benchmark configurations
benchmarks:
  - case_type: accuracy
    dataset_path: vllm-ascend/gsm8k-lite
    request_conf: vllm_api_general_chat
    dataset_conf: gsm8k/gsm8k_gen_0_shot_cot_chat_prompt
    max_out_len: 4096
    batch_size: 8
    baseline: 95
    threshold: 5

  - case_type: performance
    dataset_path: vllm-ascend/GSM8K-in3500-bs400
    request_conf: vllm_api_stream_chat
    dataset_conf: gsm8k/gsm8k_gen_0_shot_cot_str_perf
    num_prompts: 100
    max_out_len: 1500
    batch_size: 4
    request_rate: 11.2
    baseline: 134
    threshold: 0.97
