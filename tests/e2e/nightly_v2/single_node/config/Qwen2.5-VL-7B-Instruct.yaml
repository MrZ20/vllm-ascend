# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file is a part of the vllm-ascend project.
#

# =============================================================================
# Qwen2.5-VL-7B-Instruct Single Node Test Configuration
# =============================================================================
# This configuration runs Qwen2.5-VL-7B multimodal model on a single node.
# Supports vision-language inference.

test_name: "Qwen2.5-VL-7B-Instruct single node test"
model: "Qwen/Qwen2.5-VL-7B-Instruct"

# Single node configuration
num_nodes: 1
npu_per_node: 4

# Environment variables
env_common:
  TASK_QUEUE_ENABLE: "1"
  VLLM_ASCEND_ENABLE_NZ: "0"
  HCCL_OP_EXPANSION_MODE: "AIV"
  SERVER_PORT: 8080

# Server arguments
server_args:
  - "--no-enable-prefix-caching"
  - "--mm-processor-cache-gb"
  - "0"
  - "--tensor-parallel-size"
  - "4"
  - "--max-model-len"
  - "30000"
  - "--max-num-batched-tokens"
  - "40000"
  - "--max-num-seqs"
  - "400"
  - "--trust-remote-code"
  - "--gpu-memory-utilization"
  - "0.8"
  - "--compilation_config"
  - '{"cudagraph_mode": "FULL_DECODE_ONLY"}'

# Benchmark configurations
benchmarks:
  - case_type: accuracy
    dataset_path: vllm-ascend/textvqa-lite
    request_conf: vllm_api_stream_chat
    dataset_conf: textvqa/textvqa_gen_base64
    max_out_len: 2048
    batch_size: 128
    baseline: 82.05
    threshold: 5

  - case_type: performance
    dataset_path: vllm-ascend/textvqa-perf-1080p
    request_conf: vllm_api_stream_chat
    dataset_conf: textvqa/textvqa_gen_base64
    num_prompts: 512
    max_out_len: 256
    batch_size: 128
    request_rate: 0
    baseline: 1
    threshold: 0.97
